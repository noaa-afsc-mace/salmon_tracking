# Tracking dataset
This guide describes how to create all data for object detection model training, tracker tuning, and evaluation.

## Source data

To start, download our annotated dataset. Our dataset is stored here: [salmon and pollock trawl dataset](https://console.cloud.google.com/storage/browser/nmfs_odp_afsc/RACE/MACE/salmon_pollock_object_detection/annotated_data?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))). It is ~30GB.

## Data generation


### Convert and partition dataset into test and train sets

For model training, we will convert the COCO annotations to YOLO format as follows:
1. Convert COCO annotations to YOLO format using [`convert_coco_to_yolo.py`](tracking/tracking_data_generation/coco_to_yolo/convert_coco_to_yolo.py)
    
    See [file documentation](#convert_coco_to_yolopy) for complete documentation


2. Run [`yolo_test_train_split.py`](tracking/tracking_data_generation/merge_split/yolo_test_train_split.py) to split model training data into clip-based test and train data
    
    See [file documentation](#yolo_test_train_splitpy) for complete documentation

**Now you're ready to train some models!**

*See [object_detection.md](object_detection.md) for model training documentation*

### Split YOLO annotations by clip for evaluation [optional]

>NOTE:
>This is an optional step only required for detailed object detection analysis and replicating data for our regression plots.

In order to get detailed object detection performance evaluation at the video clip level using the existing Ultralytics functionality, we had to re-arrange our YOLO annotations into sub folders. 

1. Run [`split_yolo_annotations_clipwise.py`](split_yolo_annotations_clipwise.py) to split data and save somewhere convenient


### Create MOT data for tracker optimization and evaluation [optional]

>NOTE:
>All MOT data is already stored in this repository  in [tracking/gt/mot_challenge](tracking/gt/mot_challenge) and requires no additional processing. Refer to these instructions if you need to create your own MOT evaluation dataset for a different task. 

Save MOT data somewhere convenient and temporary.

1. Convert COCO annotations to MOT. Because we have to create MOT data for several frames rates, there are a few steps
    1. First, we use [`coco_to_mot.py`](tracking/tracking_data_generation/coco_to_mot/coco_to_mot.py) to convert our COCO annotations into MOT format. These MOT annotations are in the original 30fps format

        See [file documentation](#coco_to_motpy) for complete documentation

    2. Now we create 15, 10, and 7.5 fps versions of the MOT data with [`convert_gt.py`](tracking/tracking_data_generation/frame_rate/convert_gt.py)

        See [file documentation](#convert_gtpy) for complete documentation
    

2. Split all MOT data into test and train using [`mot_split_test_train.py`](tracking/tracking_data_generation/coco_to_mot/mot_split_test_train.py). You will need to do this for each frame rate.

    See [file documentation](#mot_split_test_trainpy) for complete documentation

3. Organize MOT data for evaluation with TrackEval. TrackEval expects a very specific file structure for tracker evaluation

    1. Take a close look at [tracking/gt/mot_challenge](tracking/gt/mot_challenge) and the [TrackEval documentation](https://github.com/JonathonLuiten/TrackEval). Basically, you need to put each test/train folder for every frame rate into the `mot_challenge` folder. All the data we just generated should be in the expected format.

    2. Lastly, we need to create seqmaps using [`make_seq_maps.py`](tracking/tracking_data_generation/coco_to_mot/make_seq_map.py), which list all the clips in each dataset. Every test/train folder of MOT data needs it's own seqmap. Place the updated seqmap files in the [`mot_challenge/seqmaps`](tracking/gt/mot_challenge/seqmaps) folder.

        See [file documentation](#make_seq_mappy) for complete documentation

Now we have all the data we need to evaluate trackers. Wow.

### Downsample videos and detections for lower frame rates

>NOTE:
>This step requires detections generated by [`predict.py`](predict.py). See [object_detection.md](object_detection.md) for documentation on generating detections.

To evaluate trackers at multiple frame rates, we downsample the original 30fps videos and detections to 15, 10, and 7.5 fps.

1. Downsample videos with [`convert_video_frame_rate.py`](tracking/tracking_data_generation/frame_rate/convert_video_frame_rate.py). Videos only need to be downsampled once per frame rate.

    See [file documentation](#convert_video_frame_ratepy) for complete documentation

2. Downsample detections with [`convert_detection_frame_rate.py`](tracking/tracking_data_generation/frame_rate/convert_detection_frame_rate.py). Detections must be downsampled for every model.

    See [file documentation](#convert_detection_frame_ratepy) for complete documentation

## File documentation

### [`make_seq_maps.py`](tracking/tracking_data_generation/coco_to_mot/make_seq_map.py)

`make_seq_map.py` makes a seqmap .txt file of all clip names in the given directory of MOT data. It will create seqmaps for the train and test dirs for multiple frames rates simultaneously. Some files paths are specific to the user and will need to be updated.

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `fps`: List of frames rates. The script expects specific naming structures.
- `MOT_DIR`: Path to directory containing MOT data. Uses string formatting. Edit with caution.
- `SAVE_PATH`: Path to save seqmaps

```
python make_seq_map.py
```

### [`convert_gt.py`](tracking/tracking_data_generation/frame_rate/convert_gt.py)

`convert_gt.py` creates new versions of MOT annotations at different frame rates. It removes tracks that are less than 2 frames in duration and removes clips that do not contain any annotations after downsampling. Some files paths are specific to the user and will need to be updated.
Downsampled annotations are saved in the parent directory of the original annotations with new name indicating frame rate.

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `GT_DATA_PATH`: Path to MOT annotations from `coco_to_mot.py`
- `FRAME_REDUCTION_FACTOR`: Downsampling rate. 1 keeps the original frame rate, 2 skips every other frame.
- Paths are configured in `config.yml`

```
python convert_gt.py
```

### [`convert_video_frame_rate.py`](tracking/tracking_data_generation/frame_rate/convert_video_frame_rate.py)

`convert_video_frame_rate.py` downsamples videos to simulate lower frame rates. Downsampled videos are saved in a new `fps-{rate}` subdirectory under `VIDEO_SAVE_DIR`.

Videos only need to be downsampled once per frame rate.

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `ORIGINAL_FRAME_RATE`: Frame rate of original videos (30)
- `FRAME_REDUCTION_FACTOR`: Downsampling rate. 1 keeps the original frame rate, 2 skips every other frame, 3 keeps every third frame, 4 keeps every fourth frame.
- `VIDEO_SAVE_DIR`: Parent directory for downsampled videos
- `ORIGINAL_VIDEO_DIR`: Path to original 30fps video clips

```
python tracking/tracking_data_generation/frame_rate/convert_video_frame_rate.py
```

### [`convert_detection_frame_rate.py`](tracking/tracking_data_generation/frame_rate/convert_detection_frame_rate.py)

`convert_detection_frame_rate.py` downsamples detection data to match lower frame rate videos. Detection files are copied and renumbered into a new `fps-{rate}_{source}` directory under `SAVE_PATH`. Runs tests automatically before processing.

Detections must be downsampled for every model.

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `ORIGINAL_DETECTION_SOURCE`: Model name corresponding to folder in `DETECTION_DATA_PATH` that contains saved detections
- `ORIGINAL_FRAME_RATE`: Frame rate of original detections (30)
- `FRAME_REDUCTION_FACTOR`: Downsampling rate. 1 keeps the original frame rate, 2 skips every other frame, 3 keeps every third frame, 4 keeps every fourth frame.
- `DETECTION_DATA_PATH`: Path to directory containing original detection data
- `SAVE_PATH`: Path to save downsampled detections

```
python tracking/tracking_data_generation/frame_rate/convert_detection_frame_rate.py
```

### [`mot_split_test_train.py`](tracking/tracking_data_generation/coco_to_mot/mot_split_test_train.py)

`mot_split_test_train.py` splits MOT data into test and train directories based on `test_train_split.csv`. It excludes clips with no salmon.

#### Known issues
None

#### Usage

```
python mot_split_test_train.py [mot_dir] [save_dir] [csv_path] [--exclude-empty]
```

#### Output format
```
    SAVE_DIR/
        train/
            ...
        test/
            ...
```

#### Arguments
`mot_dir`: Path to directory containing MOT annotations and `seqinfo.ini` files

`save_dir`: Save path

`csv_path`: Path to CSV with test/train split information

`--exclude-empty`: Exclude videos with no annotations. Defaults to `False`.

### [`coco_to_mot.py`](tracking/tracking_data_generation/coco_to_mot/coco_to_mot.py)

`coco_to_mot.py` uses the COCO dataset to create MOT format files for tracking evaluation

#### Known issues
None

#### Usage

```
coco_to_mot.py [-h] [COCO_ANNOTATION_DIR] [SAVE_DIR] [-i]
```

#### Output format
```
SAVE_DIR/
    <clip_name>/
        gt/
            gt.txt
        seqinfo.ini
    ...
```

#### Arguments
`COCO_ANNOTATION_DIR`: Path to directory containing COCO annotations

`SAVE_DIR`: Path to directory where MOT annotations will be saved

`-i, --class_of_interest`: Name of class to include in MOT data (can only be one). Defaults to "Salmon". Ex: -i Pollock

### [`yolo_test_train_split.py`](tracking/tracking_data_generation/merge_split/yolo_test_train_split.py)

`yolo_test_train_split.py` partitions YOLO annotations into test and train folders based on `test_train_split.csv`. Files are not renamed

#### Known issues
None

#### Usage

```
python yolo_test_train_split.py [-h] [SAVE_DIR] [YOLO_ANNOTATIONS_DIR] [TEST_TRAIN_SPLIT_CSV]
```

Example:
```
python3 yolo_test_train_split.py yolo_annotations/ train_test_yolo/ tracking/tracking_data_generation/data/train_test_split.csv
```

#### Output format
```
<test>/
    <vid_name>_frame_<frame_number>.txt
    <vid_name>_frame_<frame_number>.png
    ...
<train>/
    <vid_name>_frame_<frame_number>.txt
    <vid_name>_frame_<frame_number>.png
    ...
```

#### Arguments
`SAVE_DIR`: Empty directory

`YOLO_ANNOTATIONS_DIR`: Path to directory containing YOLO annotations

`TEST_TRAIN_SPLIT_CSV`: Path to csv file with test train split for each clip [`train_test_split.csv`](tracking_data_generation/data/train_test_split.csv)


### [`gen_clip_info.py`](tracking/tracking_data_generation/merge_split/gen_clip_info.py)

`gen_clip_info.py` creates as CSV file with information about all clips in the dataset with the following columns:
```"Annotation File", "Num Frames", "Num Salmon Annotations", "Num Pollock Annotations", "No Fish", "Fish Low", "Fish Med", "Fish High", "Occlusion", "Low Visibility", "Num Salmon Tracks", "Num Pollock Tracks"```

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `ANNOTATION_DIR`: path to COCO dataset
- `SAVE_DIR`: path to save CSV file

```
python gen_clip_info.py
```

### [`convert_coco_to_yolo.py`](tracking/tracking_data_generation/coco_to_yolo/convert_coco_to_yolo.py)

`convert_coco_to_yolo.py` converts COCO annotations to YOLO annotations for model training. Only includes classes of interest from COCO annotations.

#### Known issues
None

#### Usage

```
python convert_coco_to_yolo.py [COCO_ANNOTATION_DIR] [SAVE_DIR] -i
```

#### Output format
```
    SAVE_DIR/
    <vid_name>_frame_<frame_number>.txt
    <vid_name>_frame_<frame_number>.png
    ...
```

#### Arguments
`COCO_ANNOTATION_DIR`: Path to directory containing COCO annotations

`SAVE_DIR`: Path to directory where yolo annotations will be saved

`-i`: Names of classes to include in yolo data

### [`split_yolo_annotations_clipwise.py`](split_yolo_annotations_clipwise.py)

`split_yolo_annotations_clipwise.py` organizes YOLO annotations into sub-folders for each clip to allow for detailed clip-level evaluation metrics of object detection models.

#### Known issues
None

#### Usage

The following parameters need to be updated before running the script:
- `ANNOTATIONS_PATH`: Path to folder with YOLO annotations
- `ORGANIZED_PATH`: Save path for clipwise split annotations
- `CLIP_MAPPING_CSV`: Path to train_test split csv file

```
python split_yolo_annotations_clipwise.py
```